SLRMC <- SLR$samples/100 # centimeters to meters
years <- SLR$years
for( t in 1:length(targ_years) ){
slr_samps <- SLRMC[,which(years==targ_years[t])]
print(paste("The mean SLR projection for ",targ_years[t]," is: ",mean(slr_samps)," meters",sep=""))
# Future flood curves that include GPD parameter and SLR uncertainty
ans <- ESLreturn_Nslr_mixed(z,gpd_thresh,GPD[,2],GPD[,1],phi,
norm_mu,norm_sigma,slr_samps,1000)
Ne_slr[t,s,] <- apply(matrix(ans,nrow=10000,ncol=length(z)),2,mean)
} # each target year
} # Each SLR scenario
# Plot ESL curves
# Get historical ESLs to calculate empirical CDF and to add to plot
obs_dir <- paste(root,"/Ghanbari\ Mixture\ Model/Ghanbari\ Mixture\ Fit/obs_daily_max_",basin,sep="")
obs_fil <- paste(obs_dir,paste("obs_daily_max_",basin,"_gauge_",gauge_id,".tsv",sep=""),sep="/")
obs_dir <- paste(root,"/Ghanbari\ Mixture\ Model/Ghanbari\ Mixture\ Fit/obs_gpd_exceed_decluster_",basin,sep="")
obs_fil <- paste(obs_dir,paste("obs_gpd_exceed_decluster_",basin,"_gauge_",gauge_id,".tsv",sep=""),sep="/")
obs_fil
fil <- obs_fil
dat <- read.csv( fil, header=F, sep="\t")
gt <- function(x,y) length( which( x > y ) )/( length(x)/ 365.25 )
ct <- NA
for(iii in 1:length(z) ){
ct[iii] <- gt( dat$V4, z[iii] )
}
subct <- which( diff(ct) < 0 )
df <- data.frame( z=z[subct], freq=ct[subct], group="Observed" )
return ( df )
z <- seq(0, 10, .01) # some ESL heights (meters above tide gauge MHHW)
dat <- read.csv( fil, header=F, sep="\t")
gt <- function(x,y) length( which( x > y ) )/( length(x)/ 365.25 )
ct <- NA
for(iii in 1:length(z) ){
ct[iii] <- gt( dat$V4, z[iii] )
}
subct <- which( diff(ct) < 0 )
df <- data.frame( z=z[subct], freq=ct[subct], group="Observed" )
return ( df )
warnings()
dat
dat$V4
View(dat)
dat <- read.csv( fil, header=T, sep="\t")
View(dat)
ct <- NA
for(iii in 1:length(z) ){
ct[iii] <- gt( dat$Exceedance_Rel_MHHW, z[iii] )
}
subct <- which( diff(ct) < 0 )
df <- data.frame( z=z[subct], freq=ct[subct], group="Observed" )
View(df)
obs <- df
obs$group <- paste("Observed (",y1,"-",y2,")",sep="")
for( t in 1:length(targ_years) ){
freq <- c(Ne_hist,gpdCI[1,],gpdCI[2,],gpdCI[3,],Ne_slr[t,1,],Ne_slr[t,2,])
freq[is.na(freq)] <- 10e-10
name <- c(rep("Historic",length(z)),
rep("q5",length(z)),rep("q50",length(z)),rep("q95",length(z)),
rep("Ne 2.0°C",length(z)),rep("Ne 5.0°C",length(z)))
df <- data.frame(height=z,freq=freq,name=name,loc=param$Station_Name[j],
basin=basin,year=targ_years[t],id=gauge_id)
plotGPDNExceed_slr(df, obs, targ_years[t])
} # each target year
obs$group <- paste("Observed (",y1,"-",y2,")",sep="")
for( t in 1:length(targ_years) ){
freq <- c(Ne_hist,gpdCI[1,],gpdCI[2,],gpdCI[3,],Ne_slr[t,1,],Ne_slr[t,2,])
freq[is.na(freq)] <- 10e-10
name <- c(rep("Historic",length(z)),
rep("q5",length(z)),rep("q50",length(z)),rep("q95",length(z)),
rep("Ne 2.0°C",length(z)),rep("Ne 5.0°C",length(z)))
df <- data.frame(height=z,freq=freq,name=name,loc=param$Station_Name[j],
basin=basin,year=targ_years[t],id=gauge_id)
plotGPDNExceed_slr(df, obs, targ_years[t])
} # each target year
obs
z <- seq(-3,10,.01)
df <- data.frame(height=z,freq=freq,name=name,loc=param$Station_Name[j],
basin=basin,year=targ_years[t],id=gauge_id)
GPD <- GPDsample(1000, scale, shape, shapeV, scaleV, shapescaleV)
z <- seq(-3,10,.01) # some ESL heights (meters above tide gauge MHHW)
# Historical ESL return curve CDF (No SLR) (include GPD uncertainty)
qqq_ghanbari <- matrix(NaN, nrow=length(GPD[,2]), ncol=length(z))
for(iii in 1:length(GPD[,2]) ){
qqq_ghanbari[iii,] <- ESLreturn_N_mixed(z,gpd_thresh,GPD[iii,2],GPD[iii,1],phi,
norm_mu,norm_sigma)
}
# GPD confidence intervals
gpdCI <- apply(qqq_ghanbari,2,quantile,probs=c(0.05,0.5,0.95),na.rm=T)
Ne_hist <- apply(qqq_ghanbari,2,mean,na.rm=T)
Ne_slr <- array(NaN,dim=c(length(targ_years),length(scenarios),length(z))) # Effective SLR
for( s in 1:length(scenarios) ){
sub_dir = paste(scen_dir[s],"/",paste(scen_dir[s],"_",slab[s],sep=""),sep="")
# Get sea level rise Monte Carlo Samples
if (scenarios[s]=="5.0C"){
fil_slr <- paste( dir_slr,"/",sub_dir,"/LSLproj_MC_RCP85+H_",slr_id,"_rcp85+H.tsv",sep="")
}else if (scenarios[s]=="2.0C"){
fil_slr <- paste( dir_slr,"/",sub_dir,"/LSLproj_MC_2.0C+L_",slr_id,"_2p0degree+L.tsv",sep="")
}else{
fil_slr <- paste( dir_slr,"/",sub_dir,"/LSLproj_MC_",slr_id,"_",slab[s],".tsv",sep="")
}
SLR <- getRSLsamps( fil_slr, .999 )
SLRMC <- SLR$samples/100 # centimeters to meters
years <- SLR$years
for( t in 1:length(targ_years) ){
slr_samps <- SLRMC[,which(years==targ_years[t])]
print(paste("The mean SLR projection for ",targ_years[t]," is: ",mean(slr_samps)," meters",sep=""))
# Future flood curves that include GPD parameter and SLR uncertainty
ans <- ESLreturn_Nslr_mixed(z,gpd_thresh,GPD[,2],GPD[,1],phi,
norm_mu,norm_sigma,slr_samps,1000)
Ne_slr[t,s,] <- apply(matrix(ans,nrow=10000,ncol=length(z)),2,mean)
} # each target year
} # Each SLR scenario
# Put data into data frame for plotting...
for( t in 1:length(targ_years) ){
freq <- c(Ne_hist,gpdCI[1,],gpdCI[2,],gpdCI[3,],Ne_slr[t,1,],Ne_slr[t,2,])
freq[is.na(freq)] <- 10e-10
name <- c(rep("Historic",length(z)),
rep("q5",length(z)),rep("q50",length(z)),rep("q95",length(z)),
rep("Ne 2.0°C",length(z)),rep("Ne 5.0°C",length(z)))
df <- data.frame(height=z,freq=freq,name=name,loc=param$Station_Name[j],
basin=basin,year=targ_years[t],id=gauge_id)
plotGPDNExceed_slr(df, obs, targ_years[t])
} # each target year
source('~/Dropbox (Princeton)/Projects/Flood Return Curve Revisit/Ghanbari Mixture Model/ESL Return Curves Ghanbari/routines/plotGPDNexceed_slr.R')
for( t in 1:length(targ_years) ){
freq <- c(Ne_hist,gpdCI[1,],gpdCI[2,],gpdCI[3,],Ne_slr[t,1,],Ne_slr[t,2,])
freq[is.na(freq)] <- 10e-10
name <- c(rep("Historic",length(z)),
rep("q5",length(z)),rep("q50",length(z)),rep("q95",length(z)),
rep("Ne 2.0°C",length(z)),rep("Ne 5.0°C",length(z)))
df <- data.frame(height=z,freq=freq,name=name,loc=param$Station_Name[j],
basin=basin,year=targ_years[t],id=gauge_id)
plotGPDNExceed_slr(df, obs, targ_years[t])
} # each target year
source('~/Dropbox (Princeton)/Projects/Flood Return Curve Revisit/Ghanbari Mixture Model/ESL Return Curves Ghanbari/routines/ESLreturn_N_mixed.R')
# Account for GPD parameter uncertainty by making draws from a
# bivariate normal distribution using Latin hypercube sampling
GPD <- GPDsample(1000, scale, shape, shapeV, scaleV, shapescaleV)
z <- seq(-3,10,.01) # some ESL heights (meters above tide gauge MHHW)
# Historical ESL return curve CDF (No SLR) (include GPD uncertainty)
qqq_ghanbari <- matrix(NaN, nrow=length(GPD[,2]), ncol=length(z))
for(iii in 1:length(GPD[,2]) ){
qqq_ghanbari[iii,] <- ESLreturn_N_mixed(z,gpd_thresh,GPD[iii,2],GPD[iii,1],phi,
norm_mu,norm_sigma)
}
# GPD confidence intervals
gpdCI <- apply(qqq_ghanbari,2,quantile,probs=c(0.05,0.5,0.95),na.rm=T)
Ne_hist <- apply(qqq_ghanbari,2,mean,na.rm=T)
Ne_slr <- array(NaN,dim=c(length(targ_years),length(scenarios),length(z))) # Effective SLR
for( s in 1:length(scenarios) ){
sub_dir = paste(scen_dir[s],"/",paste(scen_dir[s],"_",slab[s],sep=""),sep="")
# Get sea level rise Monte Carlo Samples
if (scenarios[s]=="5.0C"){
fil_slr <- paste( dir_slr,"/",sub_dir,"/LSLproj_MC_RCP85+H_",slr_id,"_rcp85+H.tsv",sep="")
}else if (scenarios[s]=="2.0C"){
fil_slr <- paste( dir_slr,"/",sub_dir,"/LSLproj_MC_2.0C+L_",slr_id,"_2p0degree+L.tsv",sep="")
}else{
fil_slr <- paste( dir_slr,"/",sub_dir,"/LSLproj_MC_",slr_id,"_",slab[s],".tsv",sep="")
}
SLR <- getRSLsamps( fil_slr, .999 )
SLRMC <- SLR$samples/100 # centimeters to meters
years <- SLR$years
for( t in 1:length(targ_years) ){
slr_samps <- SLRMC[,which(years==targ_years[t])]
print(paste("The mean SLR projection for ",targ_years[t]," is: ",mean(slr_samps)," meters",sep=""))
# Future flood curves that include GPD parameter and SLR uncertainty
ans <- ESLreturn_Nslr_mixed(z,gpd_thresh,GPD[,2],GPD[,1],phi,
norm_mu,norm_sigma,slr_samps,1000)
Ne_slr[t,s,] <- apply(matrix(ans,nrow=10000,ncol=length(z)),2,mean)
} # each target year
} # Each SLR scenario
# Plot ESL curves
# Get historical ESLs to calculate empirical CDF and to add to plot
obs_dir <- paste(root,"/Ghanbari\ Mixture\ Model/Ghanbari\ Mixture\ Fit/obs_daily_max_",basin,sep="")
obs_fil <- paste(obs_dir,paste("obs_daily_max_",basin,"_gauge_",gauge_id,".tsv",sep=""),sep="/")
obs_dir <- paste(root,"/Ghanbari\ Mixture\ Model/Ghanbari\ Mixture\ Fit/obs_gpd_exceed_decluster_",basin,sep="")
obs_fil <- paste(obs_dir,paste("obs_gpd_exceed_decluster_",basin,"_gauge_",gauge_id,".tsv",sep=""),sep="/")
obs <- empiricalCDF(obs_fil, gpd_thresh, BELOW=TRUE) #include values below GPD threshold
obs$group <- paste("Observed (",y1,"-",y2,")",sep="")
# Put data into data frame for plotting...
for( t in 1:length(targ_years) ){
freq <- c(Ne_hist,gpdCI[1,],gpdCI[2,],gpdCI[3,],Ne_slr[t,1,],Ne_slr[t,2,])
freq[is.na(freq)] <- 10e-10
name <- c(rep("Historic",length(z)),
rep("q5",length(z)),rep("q50",length(z)),rep("q95",length(z)),
rep("Ne 2.0°C",length(z)),rep("Ne 5.0°C",length(z)))
df <- data.frame(height=z,freq=freq,name=name,loc=param$Station_Name[j],
basin=basin,year=targ_years[t],id=gauge_id)
plotGPDNExceed_slr(df, obs, targ_years[t])
} # each target year
#}# each tide gauge
source('~/Dropbox (Princeton)/Projects/Flood Return Curve Revisit/Ghanbari Mixture Model/ESL Return Curves Ghanbari/routines/empiricalCDF.R')
# Get historical ESLs to calculate empirical CDF and to add to plot
obs_dir <- paste(root,"/Ghanbari\ Mixture\ Model/Ghanbari\ Mixture\ Fit/obs_daily_max_",basin,sep="")
obs_fil <- paste(obs_dir,paste("obs_daily_max_",basin,"_gauge_",gauge_id,".tsv",sep=""),sep="/")
#obs_dir <- paste(root,"/Ghanbari\ Mixture\ Model/Ghanbari\ Mixture\ Fit/obs_gpd_exceed_decluster_",basin,sep="")
#obs_fil <- paste(obs_dir,paste("obs_gpd_exceed_decluster_",basin,"_gauge_",gauge_id,".tsv",sep=""),sep="/")
obs <- empiricalCDF(obs_fil, gpd_thresh, BELOW=TRUE) #include values below GPD threshold
obs$group <- paste("Observed (",y1,"-",y2,")",sep="")
# Put data into data frame for plotting...
for( t in 1:length(targ_years) ){
freq <- c(Ne_hist,gpdCI[1,],gpdCI[2,],gpdCI[3,],Ne_slr[t,1,],Ne_slr[t,2,])
freq[is.na(freq)] <- 10e-10
name <- c(rep("Historic",length(z)),
rep("q5",length(z)),rep("q50",length(z)),rep("q95",length(z)),
rep("Ne 2.0°C",length(z)),rep("Ne 5.0°C",length(z)))
df <- data.frame(height=z,freq=freq,name=name,loc=param$Station_Name[j],
basin=basin,year=targ_years[t],id=gauge_id)
plotGPDNExceed_slr(df, obs, targ_years[t])
} # each target year
source('~/Dropbox (Princeton)/Projects/Flood Return Curve Revisit/Ghanbari Mixture Model/ESL Return Curves Ghanbari/routines/empiricalCDF.R')
# Get historical ESLs to calculate empirical CDF and to add to plot
obs_dir <- paste(root,"/Ghanbari\ Mixture\ Model/Ghanbari\ Mixture\ Fit/obs_daily_max_",basin,sep="")
obs_fil <- paste(obs_dir,paste("obs_daily_max_",basin,"_gauge_",gauge_id,".tsv",sep=""),sep="/")
#obs_dir <- paste(root,"/Ghanbari\ Mixture\ Model/Ghanbari\ Mixture\ Fit/obs_gpd_exceed_decluster_",basin,sep="")
#obs_fil <- paste(obs_dir,paste("obs_gpd_exceed_decluster_",basin,"_gauge_",gauge_id,".tsv",sep=""),sep="/")
obs <- empiricalCDF(obs_fil, gpd_thresh, BELOW=TRUE) #include values below GPD threshold
obs$group <- paste("Observed (",y1,"-",y2,")",sep="")
# Put data into data frame for plotting...
for( t in 1:length(targ_years) ){
freq <- c(Ne_hist,gpdCI[1,],gpdCI[2,],gpdCI[3,],Ne_slr[t,1,],Ne_slr[t,2,])
freq[is.na(freq)] <- 10e-10
name <- c(rep("Historic",length(z)),
rep("q5",length(z)),rep("q50",length(z)),rep("q95",length(z)),
rep("Ne 2.0°C",length(z)),rep("Ne 5.0°C",length(z)))
df <- data.frame(height=z,freq=freq,name=name,loc=param$Station_Name[j],
basin=basin,year=targ_years[t],id=gauge_id)
plotGPDNExceed_slr(df, obs, targ_years[t])
} # each target year
source('~/Dropbox (Princeton)/Projects/Flood Return Curve Revisit/Ghanbari Mixture Model/ESL Return Curves Ghanbari/routines/empiricalCDF.R')
# Get historical ESLs to calculate empirical CDF and to add to plot
obs_dir <- paste(root,"/Ghanbari\ Mixture\ Model/Ghanbari\ Mixture\ Fit/obs_daily_max_",basin,sep="")
obs_fil <- paste(obs_dir,paste("obs_daily_max_",basin,"_gauge_",gauge_id,".tsv",sep=""),sep="/")
#obs_dir <- paste(root,"/Ghanbari\ Mixture\ Model/Ghanbari\ Mixture\ Fit/obs_gpd_exceed_decluster_",basin,sep="")
#obs_fil <- paste(obs_dir,paste("obs_gpd_exceed_decluster_",basin,"_gauge_",gauge_id,".tsv",sep=""),sep="/")
obs <- empiricalCDF(obs_fil, gpd_thresh, BELOW=TRUE) #include values below GPD threshold
obs$group <- paste("Observed (",y1,"-",y2,")",sep="")
# Put data into data frame for plotting...
for( t in 1:length(targ_years) ){
freq <- c(Ne_hist,gpdCI[1,],gpdCI[2,],gpdCI[3,],Ne_slr[t,1,],Ne_slr[t,2,])
freq[is.na(freq)] <- 10e-10
name <- c(rep("Historic",length(z)),
rep("q5",length(z)),rep("q50",length(z)),rep("q95",length(z)),
rep("Ne 2.0°C",length(z)),rep("Ne 5.0°C",length(z)))
df <- data.frame(height=z,freq=freq,name=name,loc=param$Station_Name[j],
basin=basin,year=targ_years[t],id=gauge_id)
plotGPDNExceed_slr(df, obs, targ_years[t])
} # each target year
View(param)
source('~/Dropbox (Princeton)/Projects/Flood Return Curve Revisit/Ghanbari Mixture Model/ESL Return Curves Ghanbari/return_curve_ghanbari_SLR.R')
obs_fil
source('~/Dropbox (Princeton)/Projects/Flood Return Curve Revisit/Ghanbari Mixture Model/ESL Return Curves Ghanbari/routines/empiricalCDF.R')
obs <- empiricalCDF(obs_fil, gpd_thresh, BELOW=TRUE) #include values below GPD threshold
obs$group <- paste("Observed (",y1,"-",y2,")",sep="")
# Put data into data frame for plotting...
for( t in 1:length(targ_years) ){
freq <- c(Ne_hist,gpdCI[1,],gpdCI[2,],gpdCI[3,],Ne_slr[t,1,],Ne_slr[t,2,])
freq[is.na(freq)] <- 10e-10
name <- c(rep("Historic",length(z)),
rep("q5",length(z)),rep("q50",length(z)),rep("q95",length(z)),
rep("Ne 2.0°C",length(z)),rep("Ne 5.0°C",length(z)))
df <- data.frame(height=z,freq=freq,name=name,loc=param$Station_Name[j],
basin=basin,year=targ_years[t],id=gauge_id)
plotGPDNExceed_slr(df, obs, targ_years[t])
} # each target year
source('~/Dropbox (Princeton)/Projects/Flood Return Curve Revisit/Ghanbari Mixture Model/ESL Return Curves Ghanbari/routines/ESLreturn_N_mixed.R')
source('~/Dropbox (Princeton)/Projects/Flood Return Curve Revisit/Ghanbari Mixture Model/ESL Return Curves Ghanbari/return_curve_ghanbari_SLR.R')
source('~/Dropbox (Princeton)/Projects/Flood Return Curve Revisit/Ghanbari Mixture Model/ESL Return Curves Ghanbari/routines/ESLreturn_N_mixed.R')
source('~/Dropbox (Princeton)/Projects/Flood Return Curve Revisit/Ghanbari Mixture Model/ESL Return Curves Ghanbari/return_curve_ghanbari_SLR.R')
source('~/Dropbox (Princeton)/Projects/Flood Return Curve Revisit/Ghanbari Mixture Model/ESL Return Curves Ghanbari/return_curve_ghanbari_SLR.R')
source('~/Dropbox (Princeton)/Projects/Flood Return Curve Revisit/Ghanbari Mixture Model/ESL Return Curves Ghanbari/return_curve_ghanbari_SLR.R')
source('~/Dropbox (Princeton)/Projects/Flood Return Curve Revisit/Ghanbari Mixture Model/ESL Return Curves Ghanbari/routines/plotGPDNexceed_slr.R')
View(param)
source('~/Dropbox (Princeton)/Projects/Flood Return Curve Revisit/Ghanbari Mixture Model/ESL Return Curves Ghanbari/return_curve_ghanbari_SLR.R')
site
(file.exists(fil_slr)
)
!(file.exists(fil_slr))
!file.exists(fil_slr)
# Historical ESL return curve CDF (No SLR) (include GPD uncertainty)
qqq_buchanan <- matrix(NaN, nrow=length(GPD[,2]), ncol=length(z))
for(iii in 1:length(GPD[,2]) ){
qqq_buchanan[iii,] <- GPDNExceed(z-gpd_thresh,lambda,-gpd_thresh,GPD[iii,2],GPD[iii,1],mhhw_exceed)
}
# GPD confidence intervals
#gpdCI <- apply(qqq_buchanan,2,quantile,probs=c(0.05,0.5,0.95),na.rm=T)
Ne_hist_gumbel <- apply(qqq_buchanan,2,mean,na.rm=T)
source("routines/GPDsample.R")
source("routines/ESLreturn_N_mixed.R")
source("routines/openSLR.R")
source("routines/GPDNExceed.R") # expected number of events with SLR
source("routines/GPDENExceed.R") # expected number of events with SLR
source("routines/ESLreturn_Nslr_mixed.R")
source("routines/empiricalCDF.R")
source("routines/plotGPDNexceed.R")
source("routines/plotGPDNExceed_slr.R")
# Historical ESL return curve CDF (No SLR) (include GPD uncertainty)
qqq_buchanan <- matrix(NaN, nrow=length(GPD[,2]), ncol=length(z))
for(iii in 1:length(GPD[,2]) ){
qqq_buchanan[iii,] <- GPDNExceed(z-gpd_thresh,lambda,-gpd_thresh,GPD[iii,2],GPD[iii,1],mhhw_exceed)
}
# GPD confidence intervals
#gpdCI <- apply(qqq_buchanan,2,quantile,probs=c(0.05,0.5,0.95),na.rm=T)
Ne_hist_gumbel <- apply(qqq_buchanan,2,mean,na.rm=T)
source("routines/GPDNExceed.R") # expected number of events with SLR
source("routines/GPDENExceed.R") # expected number of events with SLR
# Historical ESL return curve CDF (No SLR) (include GPD uncertainty)
qqq_buchanan <- matrix(NaN, nrow=length(GPD[,2]), ncol=length(z))
for(iii in 1:length(GPD[,2]) ){
qqq_buchanan[iii,] <- GPDNExceed(z-gpd_thresh,lambda,-gpd_thresh,GPD[iii,2],GPD[iii,1],mhhw_exceed)
}
# GPD confidence intervals
#gpdCI <- apply(qqq_buchanan,2,quantile,probs=c(0.05,0.5,0.95),na.rm=T)
Ne_hist_gumbel <- apply(qqq_buchanan,2,mean,na.rm=T)
source('~/Dropbox (Princeton)/Projects/Flood Return Curve Revisit/Ghanbari Mixture Model/ESL Return Curves Ghanbari/routines/GPDENExceed.R')
Ne_slr_norm <- array(NaN,dim=c(length(targ_years),length(scenarios),length(z))) # Effective SLR
Ne_slr_gum <- array(NaN,dim=c(length(targ_years),length(scenarios),length(z))) # Effective SLR
for( s in 1:length(scenarios) ){
sub_dir = paste(scen_dir[s],"/",paste(scen_dir[s],"_",slab[s],sep=""),sep="")
# Get sea level rise Monte Carlo Samples
if (scenarios[s]=="5.0C"){
fil_slr <- paste( dir_slr,"/",sub_dir,"/LSLproj_MC_RCP85+H_",slr_id,"_rcp85+H.tsv",sep="")
}else if (scenarios[s]=="2.0C"){
fil_slr <- paste( dir_slr,"/",sub_dir,"/LSLproj_MC_2.0C+L_",slr_id,"_2p0degree+L.tsv",sep="")
}else{
fil_slr <- paste( dir_slr,"/",sub_dir,"/LSLproj_MC_",slr_id,"_",slab[s],".tsv",sep="")
}
# Skip this site if there's no SLR data...
if (!file.exists(fil_slr)){
print(paste("Skipping...cannot find SLR data for PSMSL ID: ",slr_id,sep=""))
next
}
SLR <- getRSLsamps( fil_slr, .999 )
SLRMC <- SLR$samples/100 # centimeters to meters
years <- SLR$years
for( t in 1:length(targ_years) ){
slr_samps <- SLRMC[,which(years==targ_years[t])]
print(paste("The mean SLR projection for ",targ_years[t]," is: ",mean(slr_samps)," meters",sep=""))
# Normal-GPD
ans <- ESLreturn_Nslr_mixed(z,gpd_thresh,GPD[,2],GPD[,1],phi,
norm_mu,norm_sigma,slr_samps,1000)
Ne_slr_norm[t,s,] <- apply(matrix(ans,nrow=10000,ncol=length(z)),2,mean)
# Future flood curves that include GPD parameter and SLR uncertainty
zminusSLR <- mapply(function(x) x - slr_samps, z) - threshold
ans <- GPDENExceed(zminusSLR,lambda,-threshold,GPD[,2],GPD[,1],1000,mhhw_exceed)
Ne_slr_gum[t,s,] <- apply(matrix(exp(ans),nrow=10000,ncol=length(z)),2,mean)
} # each target year
} # Each SLR scenario
# Skip this site if there's no SLR data...
if (!file.exists(fil_slr)){
print(paste("Skipping ",site," cannot find SLR data for PSMSL ID: ",slr_id,sep=""))
next
}
if (!file.exists(fil_slr)){
print(paste("Skipping [",site,"] cannot find SLR data for PSMSL ID: ",slr_id,sep=""))
next
}
View(param)
j <- 329
# get shape factor
site <- param$Station_Name[j] # tide gauge site name
country <- param$Country[j]
if (country != "USA"){
next
}
basin <- param$Basin[j]
scale <- param$GPD_Scale50[j] # median scale parameter
shape <- param$GPD_Shape50[j] # median shape parameter
gauge_id <- param$Gauge_ID[j]
slr_id <- param$PSMSL_ID[j]
gpd_thresh <- param$GPD_Threshold[j] # GPD threshold
mhhw_exceed <- param$MHHW_Exceedances_per_Year[j] # historical expected MHHW exceedances per year
lambda <- param$GPD_Lambda[j] # mean Poisson arrival rate of threshold
shapescaleV <- param$GPD_Vscaleshape[j] # covariance of GPD scale and shape paraj=jmeters
shapeV <- param$GPD_Vshape[j] # variance of shape parameter
scaleV <- param$GPD_Vscale[j] # variance of scale parameter
phi <- param$Phi[j]
norm_mu <- param$Norm_Max_Mu[j]
norm_sigma <- param$Norm_Max_Sigma[j]
y1 <- param$Distrib_Fit_Start[j]
y2 <- param$Distrib_Fit_End[j]
# Account for GPD parameter uncertainty by making draws from a
# bivariate normal distribution using Latin hypercube sampling
GPD <- GPDsample(1000, scale, shape, shapeV, scaleV, shapescaleV)
z <- seq(-3,10,.01) # some ESL heights (meters above tide gauge MHHW)
# Historical ESL return curve CDF (No SLR) (include GPD uncertainty)
qqq_ghanbari <- matrix(NaN, nrow=length(GPD[,2]), ncol=length(z))
for(iii in 1:length(GPD[,2]) ){
qqq_ghanbari[iii,] <- ESLreturn_N_mixed(z,gpd_thresh,GPD[iii,2],GPD[iii,1],phi,
norm_mu,norm_sigma)
}
# GPD confidence intervals
gpdCI <- apply(qqq_ghanbari,2,quantile,probs=c(0.05,0.5,0.95),na.rm=T)
Ne_hist_norm <- apply(qqq_ghanbari,2,mean,na.rm=T)
# Historical ESL return curve CDF (No SLR) (include GPD uncertainty)
qqq_buchanan <- matrix(NaN, nrow=length(GPD[,2]), ncol=length(z))
for(iii in 1:length(GPD[,2]) ){
qqq_buchanan[iii,] <- GPDNExceed(z-gpd_thresh,lambda,-gpd_thresh,GPD[iii,2],GPD[iii,1],mhhw_exceed)
}
# GPD confidence intervals
#gpdCI <- apply(qqq_buchanan,2,quantile,probs=c(0.05,0.5,0.95),na.rm=T)
Ne_hist_gumbel <- apply(qqq_buchanan,2,mean,na.rm=T)
Ne_slr_norm <- array(NaN,dim=c(length(targ_years),length(scenarios),length(z))) # Effective SLR
Ne_slr_gum <- array(NaN,dim=c(length(targ_years),length(scenarios),length(z))) # Effective SLR
for( s in 1:length(scenarios) ){
sub_dir = paste(scen_dir[s],"/",paste(scen_dir[s],"_",slab[s],sep=""),sep="")
# Get sea level rise Monte Carlo Samples
if (scenarios[s]=="5.0C"){
fil_slr <- paste( dir_slr,"/",sub_dir,"/LSLproj_MC_RCP85+H_",slr_id,"_rcp85+H.tsv",sep="")
}else if (scenarios[s]=="2.0C"){
fil_slr <- paste( dir_slr,"/",sub_dir,"/LSLproj_MC_2.0C+L_",slr_id,"_2p0degree+L.tsv",sep="")
}else{
fil_slr <- paste( dir_slr,"/",sub_dir,"/LSLproj_MC_",slr_id,"_",slab[s],".tsv",sep="")
}
# Skip this site if there's no SLR data...
if (!file.exists(fil_slr)){
print(paste("Skipping [",site,"] cannot find SLR data for PSMSL ID: ",slr_id,sep=""))
next
}
SLR <- getRSLsamps( fil_slr, .999 )
SLRMC <- SLR$samples/100 # centimeters to meters
years <- SLR$years
for( t in 1:length(targ_years) ){
slr_samps <- SLRMC[,which(years==targ_years[t])]
print(paste("The mean SLR projection for ",targ_years[t]," is: ",mean(slr_samps)," meters",sep=""))
# Normal-GPD
ans <- ESLreturn_Nslr_mixed(z,gpd_thresh,GPD[,2],GPD[,1],phi,
norm_mu,norm_sigma,slr_samps,1000)
Ne_slr_norm[t,s,] <- apply(matrix(ans,nrow=10000,ncol=length(z)),2,mean)
# Future flood curves that include GPD parameter and SLR uncertainty
zminusSLR <- mapply(function(x) x - slr_samps, z) - threshold
ans <- GPDENExceed(zminusSLR,lambda,-threshold,GPD[,2],GPD[,1],1000,mhhw_exceed)
Ne_slr_gum[t,s,] <- apply(matrix(exp(ans),nrow=10000,ncol=length(z)),2,mean)
} # each target year
} # Each SLR scenario
for( t in 1:length(targ_years) ){
slr_samps <- SLRMC[,which(years==targ_years[t])]
print(paste("The mean SLR projection for ",targ_years[t]," is: ",mean(slr_samps)," meters",sep=""))
# Normal-GPD
ans <- ESLreturn_Nslr_mixed(z,gpd_thresh,GPD[,2],GPD[,1],phi,
norm_mu,norm_sigma,slr_samps,1000)
Ne_slr_norm[t,s,] <- apply(matrix(ans,nrow=10000,ncol=length(z)),2,mean)
# Future flood curves that include GPD parameter and SLR uncertainty
zminusSLR <- mapply(function(x) x - slr_samps, z) - gpd_thresh
ans <- GPDENExceed(zminusSLR,lambda,-gpd_thresh,GPD[,2],GPD[,1],1000,mhhw_exceed)
Ne_slr_gum[t,s,] <- apply(matrix(exp(ans),nrow=10000,ncol=length(z)),2,mean)
} # each target year
Ne_slr_norm[1,1,]
Ne_slr_gumbel[1,1,]
Ne_slr_guml[1,1,]
Ne_slr_gum[1,1,]
# Get historical ESLs to calculate empirical CDF and to add to plot
obs_dir <- paste(root,"/Ghanbari\ Mixture\ Model/Ghanbari\ Mixture\ Fit/obs_daily_max_",basin,sep="")
obs_fil <- paste(obs_dir,paste("obs_daily_max_",basin,"_gauge_",gauge_id,".tsv",sep=""),sep="/")
#obs_dir <- paste(root,"/Ghanbari\ Mixture\ Model/Ghanbari\ Mixture\ Fit/obs_gpd_exceed_decluster_",basin,sep="")
#obs_fil <- paste(obs_dir,paste("obs_gpd_exceed_decluster_",basin,"_gauge_",gauge_id,".tsv",sep=""),sep="/")
obs <- empiricalCDF(obs_fil, gpd_thresh, BELOW=TRUE) #include values below GPD threshold
obs$group <- paste("Observed (",y1,"-",y2,")",sep="")
t <- 1
freq <- c(Ne_hist_gum,Ne_hist_norm,gpdCI[1,],gpdCI[2,],gpdCI[3,],Ne_slr[t,1,],Ne_slr[t,2,])
freq[is.na(freq)] <- 10e-10
name <- c(rep("Gumbel-GPD",length(z)),rep("Normal-GPD",length(z)),
rep("q5",length(z)),rep("q50",length(z)),rep("q95",length(z)),
rep("Gumbel-GPD 2.0°C",length(z)),rep("Normal-GPD 2.0°C",length(z)))
df <- data.frame(height=z,freq=freq,name=name,loc=param$Station_Name[j],
basin=basin,year=targ_years[t],id=gauge_id)
freq <- c(Ne_hist_gum,Ne_hist_norm,gpdCI[1,],gpdCI[2,],gpdCI[3,],Ne_slr_gum[t,1,],Ne_slr_norm[t,1,])
freq[is.na(freq)] <- 10e-10
Ne_hist_gum <- apply(qqq_buchanan,2,mean,na.rm=T)
freq <- c(Ne_hist_gum,Ne_hist_norm,gpdCI[1,],gpdCI[2,],gpdCI[3,],Ne_slr_gum[t,1,],Ne_slr_norm[t,1,])
freq[is.na(freq)] <- 10e-10
name <- c(rep("Gumbel-GPD",length(z)),rep("Normal-GPD",length(z)),
rep("q5",length(z)),rep("q50",length(z)),rep("q95",length(z)),
rep("Gumbel-GPD 2.0°C",length(z)),rep("Normal-GPD 2.0°C",length(z)))
df <- data.frame(height=z,freq=freq,name=name,loc=param$Station_Name[j],
basin=basin,year=targ_years[t],id=gauge_id)
View(df)
plotGPDNExceed_slr(df, obs, targ_years[t])
source('~/Dropbox (Princeton)/Projects/Flood Return Curve Revisit/Ghanbari Mixture Model/ESL Return Curves Ghanbari/routines/plotGPDNexceed_slr_compare.R')
df <- data.frame(height=z,freq=freq,name=name,loc=param$Station_Name[j],
basin=basin,year=targ_years[t],id=gauge_id)
plotGPDNExceed_slr_compare(df, obs, targ_years[t])
source("routines/plotGPDNExceed_slr_compare.R")
plotGPDNExceed_slr_compare(df, obs, targ_years[t])
source('~/Dropbox (Princeton)/Projects/Flood Return Curve Revisit/Ghanbari Mixture Model/ESL Return Curves Ghanbari/routines/plotGPDNexceed_slr_compare.R')
plotGPDNExceed_slr_compare(df, obs, targ_years[t])
source('~/Dropbox (Princeton)/Projects/Flood Return Curve Revisit/Ghanbari Mixture Model/ESL Return Curves Ghanbari/routines/plotGPDNexceed_slr_compare.R')
plotGPDNExceed_slr_compare(df, obs, targ_years[t])
source('~/Dropbox (Princeton)/Projects/Flood Return Curve Revisit/Ghanbari Mixture Model/ESL Return Curves Ghanbari/routines/plotGPDNexceed_slr_compare.R')
plotGPDNExceed_slr_compare(df, obs, targ_years[t])
source('~/Dropbox (Princeton)/Projects/Flood Return Curve Revisit/Ghanbari Mixture Model/ESL Return Curves Ghanbari/routines/plotGPDNexceed_slr_compare.R')
plotGPDNExceed_slr_compare(df, obs, targ_years[t])
source('~/Dropbox (Princeton)/Projects/Flood Return Curve Revisit/Ghanbari Mixture Model/ESL Return Curves Ghanbari/routines/plotGPDNexceed_slr_compare.R')
plotGPDNExceed_slr_compare(df, obs, targ_years[t])
source('~/Dropbox (Princeton)/Projects/Flood Return Curve Revisit/Ghanbari Mixture Model/ESL Return Curves Ghanbari/routines/plotGPDNexceed_slr_compare.R')
source('~/Dropbox (Princeton)/Projects/Flood Return Curve Revisit/Ghanbari Mixture Model/ESL Return Curves Ghanbari/routines/plotGPDNexceed_slr_compare.R')
plotGPDNExceed_slr_compare(df, obs, targ_years[t])
for( t in 1:length(targ_years) ){
freq <- c(Ne_hist_gum,Ne_hist_norm,gpdCI[1,],gpdCI[2,],gpdCI[3,],Ne_slr_gum[t,1,],Ne_slr_norm[t,1,])
freq[is.na(freq)] <- 10e-10
name <- c(rep("Gumbel-GPD",length(z)),rep("Normal-GPD",length(z)),
rep("q5",length(z)),rep("q50",length(z)),rep("q95",length(z)),
rep("Gumbel-GPD 2.0°C",length(z)),rep("Normal-GPD 2.0°C",length(z)))
df <- data.frame(height=z,freq=freq,name=name,loc=param$Station_Name[j],
basin=basin,year=targ_years[t],id=gauge_id)
plotGPDNExceed_slr_compare(df, obs, targ_years[t])
} # each target year
source('~/Dropbox (Princeton)/Projects/Flood Return Curve Revisit/Ghanbari Mixture Model/ESL Return Curves Ghanbari/routines/plotGPDNexceed_slr_compare.R')
source('~/Dropbox (Princeton)/Projects/Flood Return Curve Revisit/Ghanbari Mixture Model/ESL Return Curves Ghanbari/return_curve_ghanbari_v_buchanan.R')
source('~/Dropbox (Princeton)/Projects/Flood Return Curve Revisit/Ghanbari Mixture Model/ESL Return Curves Ghanbari/routines/plotGPDNexceed_slr_compare.R')
source('~/Dropbox (Princeton)/Projects/Flood Return Curve Revisit/Ghanbari Mixture Model/ESL Return Curves Ghanbari/return_curve_ghanbari_v_buchanan.R')
source('~/Dropbox (Princeton)/Projects/Flood Return Curve Revisit/Ghanbari Mixture Model/Ghanbari Mixture Fit/run_GPDfit.R')
